# 实验报告

## Task1: 简单的神经网络运算框架的实现

本次任务要求实现一个简单的神经网络框架，使用 Python 和 NumPy，不依赖任何现有的深度学习库。框架需支持全连接层、卷积层、Batch Normalization 层，以及一个可选的任意层（我选择了 ReLU 激活函数）。我选择的是MINIST数据集，使用我构建的框架在该数据集上构建并训练了一个神经网络。

### 一、项目结构

- layers.py           # 各类神经网络层实现
- network.py          # 网络结构与训练流程
- utils.py            # 数据与辅助函数
- main.py             # 项目入口与训练主流程

### 二、实现细节

#### 1. layers.py -- 神经网络核心层

- 我决定将框架设计为模块化结构，每个网络层实现为一个独立的类，所有类继承自一个基类 `Layer`。基类定义了以下接口：

  - `forward(x)`：前向传播函数，接受输入 `x`，返回层的输出
  - `backward(grad)`：反向传播函数，接受上游的梯度 `grad`，计算并返回当前层的梯度
  - `update(lr)`：参数更新函数，根据学习率 `lr` 更新层的可学习参数

- **全连接层 FullyConnected**：  

  全连接层是神经网络中最基本的层之一，它通过权重矩阵和偏置向量对输入进行线性变换。

  - 初始化：随机初始化权重矩阵 `W`（形状为 `in_features × out_features`）和偏置向量 `b`（形状为 `out_features`）。我使用标准正态分布乘以 0.01 来初始化权重，偏置初始化为零
  - 前向传播：计算 `output = x @ W + b`
  - 反向传播：计算权重和偏置的梯度：
    - 权重梯度 `dW = x.T @ grad`
    - 偏置梯度 `db = np.sum(grad, axis=0)`
    - 输入梯度 `dx = grad @ W.T`
  - 参数更新：在 `update` 方法中，使用梯度下降更新 `W` 和 `b`：`W -= lr * dW`，`b -= lr * db`

  这个层的实现相对简单，但确保矩阵维度的匹配需要特别注意。

- **ReLU 激活**：

  - 激活函数：`y = max(0, x)`
  - 前向传播：计算 `output = x * (x > 0)`，其中 `(x > 0)` 生成一个布尔掩码，指示哪些元素大于 0
  - 反向传播：梯度传播时，只有输入大于 0 的位置才会传递梯度，即 `grad * mask`，其中 `mask` 是前向传播时保存的布尔掩码

  ReLU 的实现比较直观

- **Batch Normalization**：

  Batch Normalization（批归一化）通过对每个 mini-batch 的输入进行归一化，稳定训练过程。我的实现支持 2D 输入（适合全连接层）

  - 初始化：初始化可学习参数 `gamma`（全 1）和 `beta`（全 0），以及运行均值和运行方差（用于推理阶段）
  - 前向传播：
    - 计算批均值和批方差
    - 标准化输入：`x_hat = (x - mean) / np.sqrt(var + eps)`，其中 `eps` 是一个小常数（1e-5）防止除零
    - 输出为 `output = gamma * x_hat + beta`
    - 更新运行均值和方差（使用动量 0.9）
  - 反向传播：计算梯度涉及均值和方差的导数，公式较为复杂，我参考了标准推导：
    - 计算 `dxhat = grad * gamma`
    - 进一步计算 `dvar` 和 `dmean`，最终得到输入梯度 `dx`
    - 同时计算 `gamma` 和 `beta` 的梯度，用于参数更新

- **SoftmaxWithLoss 层**：

  这个层结合了 Softmax 激活函数和交叉熵损失，让我理解了分类任务中损失函数的设计

  - 前向传播：
    - 为数值稳定性，先对输入 `x` 减去每行最大值
    - 计算指数 ，然后归一化得到概率 
    - 损失为负对数似然：`-np.log(probs[np.arange(len(y)), y] + 1e-9).mean()`
  - 反向传播：梯度为 `dx = probs`，然后对真实标签位置减 1，并平均

#### 2. network.py -- 网络架构

我使用实现的框架构建了一个简单的网络，用于 MNIST 数据集：

- 输入层：展平的 MNIST 图像（784 维）。
- 全连接层：784 → 128。
- BatchNorm 层：归一化 128 维特征。
- ReLU 层：激活函数。
- 全连接层：128 → 10（对应 10 个数字类别）。
- 损失层：Softmax with Loss。

训练过程使用 mini-batch SGD，批量大小为 64，学习率为 0.01，训练 10 个 epoch。每次 epoch 后，在验证集上评估准确率

#### 3. utils.py -- 数据加载

 从 `mnist.pkl.gz` 加载训练、验证、测试数据

#### 4. main.py -- 训练主入口

初始化网络、训练模型、评估性能

### 四、实验效果

![image-20250630015017259](./图片/image-20250630015017259.png)

### 五、总结

在本次实现过程中，最大的挑战来自于对神经网络各个层前向传播与反向传播机制的理解和手动实现。与使用高层封装好的深度学习框架不同，这次任务要求不依赖任何现有的神经网络库，必须自己实现梯度传播的过程。尽管原理在课程中接触过，但真正动手写代码时发现，理论到实践的距离远比预期中大，需要参考很多课程代码以及网络资源。

遇到的一个挑战为，在实现文档要求中的卷积层后，我的网络训练速度特别慢，导致一直没有出实验结果，因此上面的实验报告是基于没有实现卷积层的报告。这一点是我需要完善改善的。

手动实现各个层，难点是在公式和数学上。单从公式上理解可能还不够，实际动手时需要把每一个中间变量都保存好，并用上合适的向量化操作，否则很容易在梯度计算时出错。

