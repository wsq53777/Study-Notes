# 深度学习架构

graph是网络的意思，TensorFlow的网络是静止的，pytorch的网络是动态的

动态网络与静态网络的对比：

![image-20231125153409861](./图片/image-20231125153409861.png)

![image-20231125153436582](./图片/image-20231125153436582.png)

![image-20231125153502724](./图片/image-20231125153502724.png)

![image-20231125154029712](./图片/image-20231125154029712.png)



![image-20231125180409820](./图片/image-20231125180409820.png)静态图好优化，动态图更灵活

![image-20231125154227419](./图片/image-20231125154227419.png)

动态图应用：

反馈网络

递归网络

模块化网络

# CNN Architectures

cnn 架构

![image-20231125182433586](./图片/image-20231125182433586.png)

LeNet：第一个卷积神经网络的成功应用实例，在数字识别领域的应用方面取得了成功

![image-20231125182609992](./图片/image-20231125182609992.png)

AlexNet第一个在ImageNet分类比赛获得成功的大型卷积神经网络

基础架构：

卷积层-池化层-归一化层-卷积层-池化层-归一化层……-池化层-全连接层

![image-20231125182820848](./图片/image-20231125182820848.png)

卷积层中没有bias，在全连接层中有bias

池化层没有参数，因为参数是指要被训练的权重，卷积层中有要被训练的权重，但是池化层中只是取每个小区域感受野的最大值，不需要参数

![image-20231125183717688](./图片/image-20231125183717688.png)

AlexNet第一个基于深度学习的网络

VGGNet:

基于AlexNet，使用很小的卷积核，更深的网络架构，没有使用局部归一化

![image-20231125184502353](./图片/image-20231125184502353.png)

卷积核越小，单个卷积核的参数越少，相应的需要堆叠的卷积核数量增多，越有机会尝试深层网络（深层是指网络中可训练权重层如卷积层、全连接层等的数量更多）

连续用3个3×3,步长为1的卷积核的感受野（3×3->5×5->7×7)和用一个7×7的卷积核的感受野是一样的，是输入的全部空间区域，但是3×3×C(通道数)的卷积核的参数更少

用(N-F)/stride+1可以推导出第n层的接受域rf为：  rfn=【rf1+2*(sn-s)/(s-1)】2，s≠1； rfn=【rf1+2(n-1)】2, s=1，

感受野扩大的原因：例如第二个卷积层在第一个卷积层得到的结果上计算，第一层卷积结果的3x3就覆盖了原始图像5x5的范围

 ![image-20231125185600344](./图片/image-20231125185600344.png)

GoogLeNet

使用了Inception模块，参数比VGGNet更少

![image-20231125190629040](./图片/image-20231125190629040.png)

![image-20231125190741629](./图片/image-20231125190741629.png)

![image-20231125191546396](./图片/image-20231125191546396.png)



 ![image-20231125191930865](./图片/image-20231125191930865.png)

![image-20231125192546535](./图片/image-20231125192546535.png)

原始的Inception模块计算费用大，1×1瓶颈层可以减少计算费用

1×1卷积核就像对特征图进行了一次线性组合，维持了空间维度的一致，但是能够减小深度通过

![image-20231125192935539](./图片/image-20231125192935539.png)

辅助分类器：

不是为了让类获得更好的分类性能，而是能够将额外的梯度直接注入到网络下层

ResNet残差网络

特点：使用Batch Normalization，局部拓扑网络

![image-20231125195259808](./图片/image-20231125195259808.png)

![image-20231125195230119](./图片/image-20231125195230119.png)

![image-20231125195900186](./图片/image-20231125195900186.png)

![image-20231125200120182](./图片/image-20231125200120182.png)

残差连接可以为梯度提供一个“高速公路”，使整个梯度在整个网络进行反向传播的速度更快，使网络更容易、更快地训练，即使网络有几百层，也可以很好地收敛，解决了深层网络弱化的问题

![image-20231125205647139](./图片/image-20231125205647139-1700917007588-1.png)

AlexNet和VGGNet最后使用的是全连接层，占主要的参数量；图中其他网络最后使用的是平均池化层，可以减少参数两

# 循环神经网络（recurrent neural network）

 ![image-20231125210018283](./图片/image-20231125210018283.png)

循环神经网络的架构：一对一、一对多、多对一、多对多，一指固定不变长度，多指可变长度

RNN是多对多的架构，但对有固定输入大小和固定输出大小的问题也很有用

![image-20231125211820754](./图片/image-20231125211820754.png)

多对一问题：

![image-20231125211849116](./图片/image-20231125211849116.png)

![image-20231125211920772](./图片/image-20231125211920772.png)

![image-20231126085350964](./图片/image-20231126085350964.png)

每次基于上一个时间步内预测得到的概率分布，在下一个时间部内生成新的预测结果，不断重复来生成一个新的序列

并不每次预测直接取概率最高的字符输出，而是基于softmax输出的概率分布进行采样（高概率的字符自然更易被采样并输出，但低概率的也有几率被输出，从而增加了输出的不确定性/多样性）

在测试阶段，通常输入的是one-hot编码向量，通常是稀疏向量而不是密集向量

当前时刻的结果依赖于前面所有时间的输入



反向传播：

沿时间的截断反向传播方法

前向计算持续一定数量的时间步，使用这些状态的隐藏值，然后反向传播同样的时间步，类似于minibatch

![image-20231126090822126](./图片/image-20231126090822126.png)

训练过程示例：

![image-20231126131520819](./图片/image-20231126131520819.png)

RNN可以通过结合图像和计算机视觉、自然语言处理解决复杂的问题，例如描述图像、补充文本、回答问题等。





多层循环网络：

![image-20231126135632427](./图片/image-20231126135632427.png)

![image-20231126140337473](./图片/image-20231126140337473.png)

反向计算RNN梯度流的时候，会不断乘以W权重矩阵的逆矩阵，这样如果W的奇异值＜1，就会产生梯度消失的问题；如果W的值＞1，就会产生梯度爆炸的问题

解决梯度爆炸的问题的方法是使用梯度截断算法，如果梯度L2范式大于某个阈值，就将它截断并做除法，这样梯度就有最大阈值；

解决梯度消失问题的做法是，使用更加复杂的RNN结构



LSTM 长短期记忆网络，用来解决梯度爆炸和梯度消失的问题

![image-20231126140759437](./图片/image-20231126140759437.png)

![image-20231126141030711](./图片/image-20231126141030711.png)

![image-20231126143311098](./图片/image-20231126143311098.png)

遗忘门在每个时间点都会发生变化，并且反向梯度流做的是矩阵元素相乘而不是元素相乘，比不断地乘以相同的权重矩阵更好，可以避免出现梯度消失和梯度爆炸的问题，并且遗忘门使用的是sigmoid函数，值在0-1之间，效果更好

单元加法或乘法可以为梯度流提供一个类似残差网络中的高速公路

![image-20231126143642510](./图片/image-20231126143642510.png)

![image-20231126143800742](./图片/image-20231126143800742.png)
